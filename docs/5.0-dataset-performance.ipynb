{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230d943a",
   "metadata": {},
   "source": [
    "# Dataset Performance\n",
    "\n",
    "This notebook evaluates the three SimpleCNN models for each task using three benchmark datasets: [PlantVillage](https://doi.org/10.48550/arXiv.1511.08060), [PlantDoc](https://doi.org/10.1145/3371158.3371196), and [DiaMOS](https://doi.org/10.3390/agronomy11112107). We present and analyze the performance metrics Accuracy and F1 Score for each model–dataset combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625af29",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26537e6",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#  Hiding some imports for brevity\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.architectures import SimpleCNN\n",
    "from lib.config import Directories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88a93e",
   "metadata": {},
   "source": [
    "We import the necessary datasets for each task. Although they are sourced from the same directory `DATASETS_DIR`, the classification criterias have been specified for each task and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = Directories.EXTERNAL_DATA_DIR.value / \"huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2000e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data import (\n",
    "    CombinedDiamosDataset,\n",
    "    CombinedPlantDocDataset,\n",
    "    CombinedPlantVillageDataset,\n",
    "    DiamosDiseaseDetection,\n",
    "    DiamosSymptomIdentification,\n",
    "    PlantDocDiseaseDetection, \n",
    "    PlantDocSymptomIdentification, \n",
    "    PlantVillageDiseaseDetection,\n",
    "    PlantVillageSymptomIdentification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94fdea",
   "metadata": {},
   "source": [
    "We define a basic resizing preprocessing procedure, applied during dataset loading, to ensure architectural compatibility with varying image dimensions. All images are normalized to a fixed square size of 32×32 pixels. We then turn the images into tensor data structures, so [PyTorch](10.48550/arXiv.1912.01703) can easily interface with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d358e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37640f40",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "We aim to evaluate the trained models on each dataset per task, to know how well MegaPlant can support models to generalize better. We restore the trained model weights from disk by loading them into an identical model architecture, then perform tests on each dataset with no data splits.\n",
    "\n",
    ":::{tip}\n",
    "To restore the trained model weights, we simply use the `torch.load()` function that returns a `dict` of the model weights, which takes in a `FilePathLike` object, `pathlib.Path` or simply a `str` representation of the file path. The model weights must be loaded into a model with the same architecture as it was trained with.\n",
    "\n",
    "```python\n",
    "simplecnn = SimpleCNN(...)\n",
    "\n",
    "simplecnn_model_weights: dict = torch.load(\"path/to/model.pth\")\n",
    "simplecnn.load_state_dict(simplecnn_model_weights)\n",
    "```\n",
    "\n",
    "Conceptually, this is similar to transferring the \"soul\" (the learned parameters) from the trained model into a newly constructed body with the same structure.\n",
    ":::\n",
    "\n",
    "We evaluate model performance using the F1-score and accuracy, as these metrics are the most commonly reported in prior work on plant disease image classification, thereby ensuring direct comparability and reproducibility of our results. Accuracy quantifies the overall proportion of correctly classified samples and is defined as the ratio of true predictions to the total number of predictions. The F1-score provides a balanced assessment of a model’s predictive capability by combining precision and recall into a single harmonic mean, making it particularly suitable for datasets with class imbalance.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "In these metrics, true positives (TP) refer to samples that are correctly classified as belonging to the positive class, true negatives (TN) denote samples correctly classified as belonging to the negative class, false positives (FP) represent negative samples that are incorrectly classified as positive, and false negatives (FN) indicate positive samples that are incorrectly classified as negative. These quantities form the basis for computing accuracy, precision, recall, and the F1-score. We will be using the [scikit-learn](https://scikit-learn.org/stable/) Python framework by @Pedregosa_Scikit-learn_Machine_Learning_2011 to calculate the evaluation metrics for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8154a3",
   "metadata": {},
   "source": [
    "## Disease Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "disease_detection_model = SimpleCNN(channels=3, output_dim=1)\n",
    "disease_detection_model.load_state_dict(torch.load(Directories.MODELS_DIR.value / \"disease_detection_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99def93",
   "metadata": {},
   "source": [
    "We specify the testing procedure in a function `test` for binary classification. This function evaluates a trained PyTorch model on a given dataset using the provided DataLoader. It computes the model’s performance by calculating both the F1 score and accuracy, and returns these two metrics as a tuple. The model argument represents the neural network to be evaluated, while the DataLoader supplies the evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module, data_loader: DataLoader) -> Tuple:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data loader and return F1 score and accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained model to evaluate.\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for the dataset to evaluate on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing F1 score and accuracy.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    THRESHOLD = 0.5\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=f\"Evaluating Dataset {data_loader.dataset.__class__.__name__}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = (outputs >= THRESHOLD).long()\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return f1, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b518142",
   "metadata": {},
   "source": [
    "### PlantVillage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ccf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset PlantVillageDiseaseDetection: 100%|██████████| 1698/1698 [00:32<00:00, 52.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9901\n",
      "Accuracy: 0.9856\n"
     ]
    }
   ],
   "source": [
    "plantvillage = PlantVillageDiseaseDetection(data_path=DATASETS_DIR / \"plantvillage\", transforms=transform_pipeline)\n",
    "plantvillage_loader = DataLoader(plantvillage, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "f1, accuracy = test(disease_detection_model, plantvillage_loader)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ab2df",
   "metadata": {},
   "source": [
    "### PlantDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5b725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset PlantDocDiseaseDetection: 100%|██████████| 92/92 [00:27<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8954\n",
      "Accuracy: 0.8392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "plantdoc = PlantDocDiseaseDetection(data_path=DATASETS_DIR / \"plantdoc\", transforms=transform_pipeline)\n",
    "plantdoc_loader = DataLoader(plantdoc, batch_size=32, shuffle=True)\n",
    "\n",
    "f1, accuracy = test(disease_detection_model, plantdoc_loader)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92a5d6",
   "metadata": {},
   "source": [
    "### DiaMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87d738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset DiamosDiseaseDetection: 100%|██████████| 94/94 [03:48<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9918\n",
      "Accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "diamos = DiamosDiseaseDetection(data_path=DATASETS_DIR / \"diamos\", transforms=transform_pipeline)\n",
    "diamos_loader = DataLoader(diamos, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "f1, accuracy = test(disease_detection_model, diamos_loader)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8fc95",
   "metadata": {},
   "source": [
    "## Symptom Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e00ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symptom_identifier = SimpleCNN(channels=3, output_dim=12)\n",
    "symptom_identifier.load_state_dict(torch.load(Directories.MODELS_DIR.value / \"symptom_identification_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e138ac",
   "metadata": {},
   "source": [
    "We specify a new test procedure `test_si` for multi-class classification tasks. It takes in the same input as the `test` function but outputs a `tuple` of `list` of targets and `list` of predictions. So that downstream, we can both calculate the F1 and Accuracy score and generate a classification report, which generates F1 score for each predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_si(model: torch.nn.Module, data_loader: DataLoader) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Evaluate the symptom identification model on the given data loader and return predictions and labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained symptom identification model to evaluate.\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for the dataset to evaluate on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing lists of true labels and predicted labels.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=f\"Evaluating Dataset {data_loader.dataset.__class__.__name__}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac52465",
   "metadata": {},
   "source": [
    "### PlantDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset PlantDocSymptomIdentification: 100%|██████████| 66/66 [00:16<00:00,  4.01it/s]\n"
     ]
    }
   ],
   "source": [
    "plantdoc_si = PlantDocSymptomIdentification(data_path=DATASETS_DIR / \"plantdoc\", transforms=transform_pipeline)\n",
    "plantdoc_si_loader = DataLoader(plantdoc_si, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "all_labels, all_preds = test_si(symptom_identifier, plantdoc_si_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666414ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7692\n",
      "Accuracy: 0.7569\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0bb741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       769\n",
      "           1       0.84      0.80      0.82       238\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.68      0.68      0.68       130\n",
      "           4       0.03      1.00      0.06         2\n",
      "           5       0.74      0.67      0.71        91\n",
      "           6       0.80      0.52      0.63        54\n",
      "           7       0.75      0.56      0.64        79\n",
      "           8       0.77      0.77      0.77       223\n",
      "           9       0.83      0.61      0.70        93\n",
      "          10       0.72      0.74      0.73       415\n",
      "          11       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.76      2094\n",
      "   macro avg       0.58      0.60      0.55      2094\n",
      "weighted avg       0.79      0.76      0.77      2094\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47037b1c",
   "metadata": {},
   "source": [
    "### PlantVillage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset PlantVillageSymptomIdentification: 100%|██████████| 1226/1226 [00:22<00:00, 54.17it/s]\n"
     ]
    }
   ],
   "source": [
    "plantvillage_si = PlantVillageSymptomIdentification(data_path=DATASETS_DIR / \"plantvillage\", transforms=transform_pipeline)\n",
    "plantvillage_si_loader = DataLoader(plantvillage_si, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "all_labels, all_preds = test_si(symptom_identifier, plantvillage_si_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759adf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9302\n",
      "Accuracy: 0.9305\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf46ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86      6970\n",
      "           1       0.99      0.99      0.99      5507\n",
      "           2       0.99      0.98      0.98      5357\n",
      "           3       0.99      0.96      0.97      2887\n",
      "           4       0.97      0.85      0.91      1676\n",
      "           5       0.97      0.87      0.92       952\n",
      "           6       0.98      0.88      0.93       373\n",
      "           7       0.96      0.95      0.96      1801\n",
      "           8       0.97      0.97      0.97      1467\n",
      "           9       0.92      0.92      0.92       630\n",
      "          10       0.83      0.98      0.90     10492\n",
      "          11       0.98      0.96      0.97      1109\n",
      "\n",
      "    accuracy                           0.93     39221\n",
      "   macro avg       0.96      0.92      0.94     39221\n",
      "weighted avg       0.94      0.93      0.93     39221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8f9ab",
   "metadata": {},
   "source": [
    "### DiaMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset DiamosSymptomIdentification: 100%|██████████| 93/93 [03:34<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "diamos_si = DiamosSymptomIdentification(data_path=DATASETS_DIR / \"diamos\", transforms=transform_pipeline)\n",
    "diamos_si_loader = DataLoader(diamos_si, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "all_labels, all_preds = test_si(symptom_identifier, diamos_si_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c91ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8919\n",
      "Accuracy: 0.8853\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ab35d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.69      0.81        54\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.90      0.95      0.93      2025\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.90      0.75      0.82       884\n",
      "\n",
      "    accuracy                           0.89      2963\n",
      "   macro avg       0.28      0.24      0.26      2963\n",
      "weighted avg       0.90      0.89      0.89      2963\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cc5d6",
   "metadata": {},
   "source": [
    "## Combined Identification and Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d6811d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comb = test_si\n",
    "\n",
    "combined_classifier = SimpleCNN(channels=3, output_dim=13)\n",
    "combined_classifier.load_state_dict(torch.load(Directories.MODELS_DIR.value / f\"combined_identification_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79811c8f",
   "metadata": {},
   "source": [
    "### PlantDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_plantdoc = CombinedPlantDocDataset(data_path=DATASETS_DIR / \"plantdoc\", transforms=transform_pipeline)\n",
    "combined_plantdoc_loader = DataLoader(combined_plantdoc, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "all_labels, all_preds = test_comb(combined_classifier, combined_plantdoc_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ab4624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7897\n",
      "Accuracy: 0.7754\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a7bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       769\n",
      "           1       0.80      0.74      0.77       238\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.87      0.75      0.81       130\n",
      "           4       0.02      1.00      0.04         2\n",
      "           5       0.76      0.56      0.65        91\n",
      "           6       0.80      0.44      0.57        54\n",
      "           7       0.66      0.72      0.69        79\n",
      "           8       0.79      0.75      0.77       223\n",
      "           9       0.71      0.61      0.66        93\n",
      "          10       0.69      0.70      0.69       415\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.88      0.84      0.86       822\n",
      "\n",
      "    accuracy                           0.78      2916\n",
      "   macro avg       0.60      0.61      0.56      2916\n",
      "weighted avg       0.81      0.78      0.79      2916\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4f012e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9242112482853223"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_for_combined = []\n",
    "preds_for_combined = []\n",
    "\n",
    "for label, pred in zip(all_labels, all_preds):\n",
    "    if label == combined_plantdoc.CLASS_MAP['healthy']:\n",
    "        labels_for_combined.append(0)\n",
    "        if pred != combined_plantdoc.CLASS_MAP['healthy']:\n",
    "            preds_for_combined.append(1)\n",
    "        else:\n",
    "            preds_for_combined.append(0)\n",
    "\n",
    "    else:\n",
    "        labels_for_combined.append(1)\n",
    "        if pred != combined_plantdoc.CLASS_MAP['healthy']:\n",
    "            preds_for_combined.append(1)\n",
    "        else:\n",
    "            preds_for_combined.append(0)\n",
    "\n",
    "accuracy_score(labels_for_combined, preds_for_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43245a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947717057014431"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_for_combined, preds_for_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67dfb5f",
   "metadata": {},
   "source": [
    "### PlantVillage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f380de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset CombinedPlantVillageDataset: 100%|██████████| 1698/1698 [00:40<00:00, 42.18it/s]\n"
     ]
    }
   ],
   "source": [
    "combined_plantvillage = CombinedPlantVillageDataset(data_path=DATASETS_DIR / \"plantvillage\", transforms=transform_pipeline)\n",
    "combined_plantvillage_loader = DataLoader(combined_plantvillage, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "all_labels, all_preds = test_comb(combined_classifier, combined_plantvillage_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7aac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9475\n",
      "Accuracy: 0.9478\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8ec09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.87      6970\n",
      "           1       0.99      0.99      0.99      5507\n",
      "           2       0.99      0.99      0.99      5357\n",
      "           3       0.99      0.97      0.98      2887\n",
      "           4       0.97      0.89      0.93      1676\n",
      "           5       0.96      0.93      0.94       952\n",
      "           6       0.88      0.95      0.91       373\n",
      "           7       0.94      0.94      0.94      1801\n",
      "           8       0.98      0.97      0.97      1467\n",
      "           9       0.88      0.91      0.89       630\n",
      "          10       0.86      0.95      0.90     10492\n",
      "          11       0.97      0.98      0.98      1109\n",
      "          12       0.98      0.98      0.98     15084\n",
      "\n",
      "    accuracy                           0.95     54305\n",
      "   macro avg       0.95      0.94      0.94     54305\n",
      "weighted avg       0.95      0.95      0.95     54305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75bde5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905349415339287"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_for_combined = []\n",
    "preds_for_combined = []\n",
    "\n",
    "for label, pred in zip(all_labels, all_preds):\n",
    "    if label == combined_plantdoc.CLASS_MAP['healthy']:\n",
    "        labels_for_combined.append(0)\n",
    "        if pred != combined_plantdoc.CLASS_MAP['healthy']:\n",
    "            preds_for_combined.append(1)\n",
    "        else:\n",
    "            preds_for_combined.append(0)\n",
    "\n",
    "    else:\n",
    "        labels_for_combined.append(1)\n",
    "        if pred != combined_plantdoc.CLASS_MAP['healthy']:\n",
    "            preds_for_combined.append(1)\n",
    "        else:\n",
    "            preds_for_combined.append(0)\n",
    "\n",
    "accuracy_score(labels_for_combined, preds_for_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bba399c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9934437102987321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_for_combined, preds_for_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392c0ec",
   "metadata": {},
   "source": [
    "### DiaMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset CombinedDiamosDataset: 100%|██████████| 94/94 [03:32<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "combined_diamos = CombinedDiamosDataset(data_path=DATASETS_DIR / \"diamos\", transforms=transform_pipeline)\n",
    "combined_diamos_loader = DataLoader(combined_diamos, batch_size=32, shuffle=True)\n",
    "\n",
    "all_labels, all_preds = test_comb(combined_classifier, combined_diamos_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "529e17fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8705\n",
      "Accuracy: 0.8666\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e7e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.72      0.84        54\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.89      0.94      0.91      2025\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.88      0.71      0.79       884\n",
      "          12       0.59      0.67      0.63        43\n",
      "\n",
      "    accuracy                           0.87      3006\n",
      "   macro avg       0.28      0.25      0.26      3006\n",
      "weighted avg       0.88      0.87      0.87      3006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/iragca/Documents/github/DS413-final-project/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfb75de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9886892880904857"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_for_combined = []\n",
    "preds_for_combined = []\n",
    "\n",
    "for label, pred in zip(all_labels, all_preds):\n",
    "    if label == combined_plantdoc.CLASS_MAP['healthy']:\n",
    "        labels_for_combined.append(0)\n",
    "        if pred != combined_plantdoc.CLASS_MAP['healthy']:\n",
    "            preds_for_combined.append(1)\n",
    "        else:\n",
    "            preds_for_combined.append(0)\n",
    "\n",
    "    else:\n",
    "        labels_for_combined.append(1)\n",
    "        if pred != combined_plantdoc.CLASS_MAP['healthy']:\n",
    "            preds_for_combined.append(1)\n",
    "        else:\n",
    "            preds_for_combined.append(0)\n",
    "\n",
    "accuracy_score(labels_for_combined, preds_for_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06b95eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9942567567567567"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_for_combined, preds_for_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6cc07",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b9cdd",
   "metadata": {},
   "source": [
    "## Disease Detection\n",
    "\n",
    "| Dataset      | F1 Score | Accuracy |\n",
    "| ------------ | -------- | -------- |\n",
    "| DiaMOS       | 0.9918   | 0.9837   |\n",
    "| PlantVillage | 0.9901   | 0.9856   |\n",
    "| PlantDoc     | 0.8954   | 0.8392   |\n",
    "\n",
    "We get small variance in performance between the 3 datasets as well very high scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ca393",
   "metadata": {},
   "source": [
    "## Symptom Identification\n",
    "\n",
    "| Dataset      | F1 Score | Accuracy |\n",
    "| ------------ | -------- | -------- |\n",
    "| DiaMOS       | 0.8919   | 0.8853   |\n",
    "| PlantVillage | 0.9302   | 0.9305   |\n",
    "| PlantDoc     | 0.7692   | 0.7569   |\n",
    "\n",
    "Considering that symptom identification is harder task, we get more variance and less overall accuracy in all three datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b59fd",
   "metadata": {},
   "source": [
    "## Combined Identification and Detection\n",
    "\n",
    "| Dataset      | F1 Score | Accuracy | Binary F1 Score | Binary Accuracy |\n",
    "| ------------ | -------- | -------- | --------------- | --------------- |\n",
    "| DiaMOS       | 0.8705   | 0.8666   | 0.9942          | 0.9886          |\n",
    "| PlantVillage | 0.9475   | 0.9478   | 0.9934          | 0.9905          |\n",
    "| PlantDoc     | 0.7897   | 0.7754   | 0.9477          | 0.9242          |\n",
    "\n",
    "Binary F1 Score and Accuracy are derived from the model’s outputs, where all symptom-present cases are mapped to the unhealthy/diseased class (1), and all healthy cases are mapped to the healthy class (0). This makes it easier to compare against disease detection models.\n",
    "\n",
    "There is no noticeable difference when compared to symptom identification task, however when we compare it with the disease detection model, we get a noticeable increase in F1 and Accuracy score, particularly with PlantDoc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7782666",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We evaluated the three task-specific SimpleCNN models on each dataset and compared their performance across tasks. From this analysis, we observed that the combined-task model may outperform the disease-detection model, although it centralizes the decision-making responsibility, which can introduce its own trade-offs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
